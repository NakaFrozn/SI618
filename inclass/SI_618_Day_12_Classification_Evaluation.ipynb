{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SI 618 Classification III (Evaluation and Application)\n",
    "### Dr. Chris Teplovs, School of Information, University of Michigan\n",
    "Copyright &copy; 2024.  This notebook may not be shared outside of the course without permission.\n",
    "### Please ensure you have this version:\n",
    "Version 2024.11.14.2.CT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will review and dive deeper into the evaluation of classifiers.\n",
    "\n",
    "First, let's import all the functionality we'll need in this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import cross_val_predict, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier # to compare with logistic regression\n",
    "\n",
    "from sklearn.metrics import auc, confusion_matrix, classification_report, precision_recall_curve, roc_curve, roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now let's read the data file, taken from Kaggle's Titanic Disaster Machine Learning page (you might need to adjust the path name to get to where you put the data file):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic = pd.read_csv('../data/titanic.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do a train-test split, retaining 20% for our test dataset.  Note that in some cases below we'll use cross-validation on the data from our training dataset, which is unnecessarily limiting, but included for the purposes of demonstration.  I'm dropping a lot of columns to make the analysis easy, but at the expense of accuracy (i.e., I'm throwing away a lot of information)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    # Aggressively drop columns for now.  Note the resultant decrease in accuracy\n",
    "    titanic.drop(['Survived', 'PassengerId', 'Name', 'Ticket', 'Cabin', 'Embarked'], axis=1), \n",
    "    titanic['Survived'], \n",
    "    test_size=0.2, \n",
    "    random_state=42)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's pre-process our data by imputing missing numeric values and then scaling all numeric variables.  We'll one-hot encode our only remaining string variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pipeline = Pipeline([\n",
    "    ('impute',SimpleImputer(strategy='median')), \n",
    "    ('scale',StandardScaler())\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_pipeline = ColumnTransformer([\n",
    "    ('num', num_pipeline, ['Age', 'Fare']),\n",
    "    ('cat', OneHotEncoder(), ['Sex'])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit and transform the training data; use the fitted pipeline to transform the test data (i.e., we do not `fit` to the test data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_prepared = preprocessing_pipeline.fit_transform(X_train)\n",
    "X_test_prepared = preprocessing_pipeline.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's fit a classifier and print out the accuracy score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression()\n",
    "\n",
    "clf.fit(X_train_prepared, y_train)\n",
    "print(clf.score(X_test_prepared, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a reminder of some definitions from last class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\text{accuracy} = \\frac{\\text{True Positives + True Negatives}}{\\text{All Samples}}$\n",
    "\n",
    "$\\text{precision} = \\frac{\\text{True Positives}}{\\text{True Positives + False Positives}}$\n",
    "\n",
    "$\\text{true positive rate} = \\text{recall} = \\text{sensitivity} = \\frac{\\text{True Positives}}{\\text{True Positives + False Negatives}}$\n",
    "\n",
    "$\\text{F1} = \\frac{2 \\times (\\text{Precision} \\times \\text{Recall})}{\\text{Precision + Recall}}$\n",
    "\n",
    "$ \\text{false positive rate} = \\text{fall-out} = \\frac{\\text{False Positives}}{\\text{False Positives + True Negatives}}$\n",
    "\n",
    "$ \\text{specificity} = \\frac{\\text{True Negatives}}{\\text{True Negatives + False Positives}}$\n",
    "\n",
    "$ \\text{false positive rate} = 1 - \\text{specificity}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confusion matrix helps us understand the relationships between the different \n",
    "states:\n",
    "```\n",
    "[[TP FN]\n",
    " [FP TN]]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_test, clf.predict(X_test_prepared)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in the above confusion matrix, the first row is the \"did not survive\" class )(i.e., Survived = 0).  That's why we have different values for precision and recall in the classification report:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, clf.predict(X_test_prepared)))\n",
    "# Note that in binary classification, recall of the positive class is also known as “sensitivity”; recall of the negative class is “specificity”."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the probabilities associated with each of the class assignments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_probabilities = cross_val_predict(clf, X_train_prepared, y_train, cv=3,\n",
    "                                    method=\"predict_proba\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can improve the output a bit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with np.set_printoptions(precision=3, suppress=True):\n",
    "    print(y_probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_scores = y_probabilities[:, 1]   # score = proba of positive class\n",
    "fpr, tpr, roc_thresholds = roc_curve(y_train,y_scores,pos_label=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curve(fpr, tpr, label=None):\n",
    "    plt.plot(fpr, tpr, linewidth=2, label=label)\n",
    "    plt.plot([0, 1], [0, 1], 'k--') # dashed diagonal\n",
    "    plt.axis([0, 1, 0, 1])                                    # Not shown in the book\n",
    "    plt.xlabel('False Positive Rate (Fall-Out)', fontsize=14) # Not shown\n",
    "    plt.ylabel('True Positive Rate (Recall)', fontsize=14)    # Not shown\n",
    "    plt.grid(True)                                            # Not shown\n",
    "\n",
    "plot_roc_curve(fpr, tpr)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"ROCAUC score: {roc_auc_score(y_train, y_scores):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precisions, recalls, pr_thresholds = precision_recall_curve(y_train,y_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_precision_recall_vs_threshold(precisions, recalls, thresholds):\n",
    "    plt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\", linewidth=2)\n",
    "    plt.plot(thresholds, recalls[:-1], \"g-\", label=\"Recall\", linewidth=2)\n",
    "    plt.legend(loc=\"center right\", fontsize=16) # Not shown in the book\n",
    "    plt.xlabel(\"Threshold\", fontsize=16)        # Not shown\n",
    "    plt.grid(True)                              # Not shown\n",
    "\n",
    "plt.figure(figsize=(8, 4))                                                                  # Not shown\n",
    "plot_precision_recall_vs_threshold(precisions, recalls, pr_thresholds)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_precision_vs_recall(precisions, recalls):\n",
    "    plt.plot(recalls, precisions, \"b-\", linewidth=2)\n",
    "    plt.xlabel(\"Recall\", fontsize=16)\n",
    "    plt.ylabel(\"Precision\", fontsize=16)\n",
    "    plt.axis([0, 1, 0, 1])\n",
    "    plt.grid(True)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plot_precision_vs_recall(precisions, recalls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Precision-recall AUC: {auc(recalls, precisions):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BREAK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part of today's class, we're going to learn about [Kaggle competitions](https://www.kaggle.com/competitions/), and we are going to start one in class: https://www.kaggle.com/competitions/titanicL.  Download the data and start your work by creating new cells below.  See if you can get a classifier to run!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
