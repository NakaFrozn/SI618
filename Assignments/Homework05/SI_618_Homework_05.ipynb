{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SI 618 Homework 5 - Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The total score for this assignment will be 100 points, consisting of:\n",
    "- 10 pt: Overall quality of spelling, grammar, punctuation, etc. of written sentences.\n",
    "- 10 pt: Code is written in [PEP 8](https://www.python.org/dev/peps/pep-0008/) style.\n",
    "- 80 pt: Homework questions. Questions 1 through 6 are worth 10 points each; Question 7 is worth 20 points.\n",
    "\n",
    "Version 2024.02.20.CT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "Recently on September 10th, 2024, there was the first U.S. Presidential Debate between two candidates, Former President Donald J. Trump and current Vice President Kamala Harris. \n",
    "\n",
    "The debate was hosted by ABC News and moderated by David Muir and Linsey Davis. This debate was held in Philadelphia, Pennsylvania in a closed room setting but aired on national television. At a time where the Presidential Election is justt around the corner and has a domestic and international spotlight on it, thhis homework will focus on how to analyze the text data from recent news sources.\n",
    "\n",
    "There are two sources of the debate transcript that are provided for this homnework. The main and primary source of the debate transcript is from The American Presidency Project at the University of California, Santa Barbara. The secondary source is from the ABC News website.\n",
    "\n",
    "\n",
    "- Link: https://www.presidency.ucsb.edu/documents/presidential-debate-philadelphia-pennsylvania\n",
    "- Link: https://abcnews.go.com/Politics/harris-trump-presidential-debate-transcript/story?id=113560542"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "In this homework, you will practice the following skills:\n",
    "\n",
    "- Getting Comfortable working with text data\n",
    "- Preprocessing techniques for text data\n",
    "- Using regular expressions to extract information from text data.\n",
    "- Tokenizing text data\n",
    "- Word filtering (stop words, punctuation, etc.)\n",
    "- Understanding the basics of Natural Language Processing (NLP)\n",
    "- Introduction to embeddings\n",
    "- Using embeddings to analyze text data\n",
    "    - Similarity\n",
    "    - Distance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“‚ Data\n",
    "\n",
    "The teaching team has provided two text files for this homework. We will only be using one of them for this assignment. \n",
    "\n",
    "- The file from the **American Presidency Project** is the primary source of data and is named **\"Full_Transcript.txt\"**.\n",
    "- The other file, from the **ABC News** website, is a partial transcript called **\"Partial_Debate_Transcript.txt\"**.\n",
    "\n",
    "###  Homework Focus:\n",
    "In this homework, you will work on skills that provide the groundwork for understanding and analyzing text data.\n",
    "\n",
    "### Extra Resource:\n",
    "The partial debate transcript is provided as an extra resource. **It is not required for this homework**, but feel free to explore it after completing the assignment.\n",
    "\n",
    "### Question for Reflection:\n",
    "Consider the following question:  \n",
    "*\"What is the impact of cherry-picked data? Do you think that a partial transcript can provide the whole truth of a debate, even if selectively chosen?\"*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please fill in your uniqname in the next code cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "MY_UNIQNAME = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer each of the questions below.  You are encouraged to use as many code and markdown cells as you need for each question.\n",
    "\n",
    "We **strongly** suggest running all cells from top to bottom before you submit your notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tag import pos_tag\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('vader_lexicon')\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Q0** - Setting Up Data Structure for Text Processing\n",
    "\n",
    "For this question, the desired output is to create a **2-column DataFrame** with the following columns:\n",
    "- **Column 1**: Speaker\n",
    "- **Column 2**: Text\n",
    "\n",
    "### Task:\n",
    "Please create a 2-column DataFrame using the data from the **\"Full_Transcript.txt\"** file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Q1.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization of the Text\n",
    "- The teaching team would like to emphasize the skill of **tokenization** of the text data.\n",
    "- **Tokenization** is the process of breaking down text data into smaller units, such as **words** or **phrases**.\n",
    "- We will use the `split()` method to tokenize the text data.\n",
    "- Using the `split()` method, please tokenize the text data for each **candidate** and **moderator**.\n",
    "- Store each in separate data structures for each candidate and moderator, along with one for the **full transcript**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may notice some irregularitites in the output of the text data, Over the next several steps we will clean the text data to make it more readable and easier to analyze."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lower all of the text data\n",
    "The next step in the NLP pipeline is to lower all of the text data. This is important because it will allow us to standardize the text data and make it easier to analyze. In this step we would like you to define a function that can be used to lower all of the text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Punctuation from the Text\n",
    "The next step in the NLP pipeline is to remove punctuation from the text data. This is another important step as it moves towards more standardization of the text data. In this step, we would like you to define a function that can be used to remove punctuation from the text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove stopwords from the text\n",
    "- The next step in the NLP pipeline is to remove stopwords from the text data. Stopwords are common words that do not provide much information about the text data. In this step we would like you to define a function that can be used to remove stopwords from the text data.\n",
    "- The teaching team has provided a list of stopwords that you can use for this step. The stopwords can be found in the stopwords.txt file.\n",
    "- To summarize, please load the stopwords.txt file and use it to remove stopwords from the text data (make use of a function for removing the stopwords)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ·ï¸ POS Tagging the Text\n",
    "\n",
    "The next step in our NLP pipeline is **Part-of-Speech (POS) tagging**.\n",
    "\n",
    "### What is POS Tagging?\n",
    "- **POS tagging** is the process of marking each word in a text with its corresponding **part of speech** based on its **definition** and **context**.\n",
    "  \n",
    "- Examples:  \n",
    "  - **Noun**: *\"dog\"*, *\"city\"*  \n",
    "  - **Verb**: *\"run\"*, *\"write\"*  \n",
    "  - **Adjective**: *\"happy\"*, *\"blue\"*\n",
    "\n",
    "### Task:\n",
    "Use the **NLTK library** to perform **POS tagging** on the text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization of the Text\n",
    "\n",
    "The final step in our NLP pipeline is **lemmatization**.\n",
    "\n",
    "### What is Lemmatization?\n",
    "- **Lemmatization** groups together the **inflected forms** of a word, so they can be analyzed as a **single item**.  \n",
    "- Example:  \n",
    "  - *\"running\"* â†’ **\"run\"**\n",
    "\n",
    "### Task:\n",
    "Define a function using the **NLTK library** to lemmatize the text data.\n",
    "\n",
    "ðŸ”— **Hint**: Use NLTKâ€™s `WordNetLemmatizer` for this process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Q2.** Word Usage Analysis by Candidate and Moderator\n",
    "\n",
    "### Key Questions:\n",
    "1. **How many words** did each **candidate** use in the debate?  \n",
    "   - *(Excluding stopwords)*\n",
    "\n",
    "2. **How many **unique words** did each **moderator** use in the debate?  \n",
    "   - *(Excluding stopwords)*\n",
    "\n",
    "### Analysis Focus:\n",
    "- **Candidates**: Calculate **total word count**.\n",
    "- **Moderators**: Calculate **unique word count**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Q3.** Counting Speaking Turns for Each Candidate and Moderator\n",
    "\n",
    "### What is a \"Turn\"?\n",
    "- A **turn** is defined as an **uninterrupted period of speech**.  \n",
    "  For example:\n",
    "  \n",
    "  > Chris: Big data is really interesting.  \n",
    "  > Colleague: Actually, it's a bit boring.  \n",
    "  > Chris: Really? Why would you say that?  \n",
    "  > Colleague: Your choice of tools is really limited.  \n",
    "  > Colleague: I mean, you're basically stuck with Spark, right?  \n",
    "  > Chris: Yeah, but Spark provides most of the functionality you need to do really cool data science work.\n",
    "\n",
    "In this example, **Chris** had 3 turns, while his **Colleague** had 2.\n",
    "\n",
    "### Desired Output:\n",
    "Format your results using a **DataFrame** with the following columns:\n",
    "\n",
    "- **Speaker**  \n",
    "- **Uninterrupted_Turns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Q4.** Analyzing Noun Usage by Each Speaker\n",
    "\n",
    "### Task:\n",
    "Calculate the **number of different nouns** used by each speaker in the debates.\n",
    "\n",
    "### Format:\n",
    "Present your answer using a **dictionary** in the following format:\n",
    "\n",
    "```python\n",
    "{\n",
    "    'DAVID MUIR': 0, \n",
    "    'LINSEY DAVIS': 0, \n",
    "    'FORMER PRESIDENT DONALD TRUMP': 0, \n",
    "    'VICE PRESIDENT KAMALA HARRIS': 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Q5.** Analyzing Unique Words Used by Each Speaker\n",
    "\n",
    "### Task:\n",
    "- **Calculate the Number of Unique Words** for each candidate and moderator.\n",
    "- Answer the following question:  \n",
    "  - How many **unique words** did each person use in the debate?\n",
    "  - What might this indicate about the **breadth of their vocabulary**?\n",
    "\n",
    "### Considerations:\n",
    "- Think about the **role** of the speaker:\n",
    "  - Are they a **candidate** trying to present their ideas?\n",
    "  - Or a **moderator** guiding the discussion?\n",
    "\n",
    "### Visualization: Zipf's Law\n",
    "- Create a **visualization** to depict **Zipf's Law** for each speaker, showing the relationship between word frequency and rank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Q6.** Calculate Bigrams by Candidate and Moderator\n",
    "\n",
    "Use **bigrams** (pairs of consecutive words) to analyze speech patterns for each candidate and the moderator.\n",
    "\n",
    "### Task:\n",
    "- Calculate **bigrams** separately for each candidate and the moderator.\n",
    "- Present the **top 5 bigrams** for each speaker.\n",
    "\n",
    "### Analysis Questions:\n",
    "- Do you notice any **interesting patterns** or unique combinations of words?\n",
    "- Are there specific phrases that are frequently used by a particular candidate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Q7.** Word Frequency Histogram\n",
    "\n",
    "Create a **word frequency histogram** for each candidate and the moderator.\n",
    "\n",
    "###  Analysis Questions:\n",
    "- Do you observe any **interesting patterns**?\n",
    "- Choose an **arbitrary word count threshold** and **remove** all words with a frequency **lower** than that threshold.\n",
    "  \n",
    "### Updated Histogram:\n",
    "- Create a **new histogram** using the updated data.\n",
    "\n",
    "###  Additional Insight:\n",
    "- Since this is a **political debate**, identify any **rare words** that might be of interest.\n",
    "- Do any moderators or candidates use **complex vocabulary**? If so, which ones?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Q8.** Sentiment Analysis of Trump vs. Harris Debate\n",
    "\n",
    "In this task, we will use the **VADER sentiment analysis tool** to evaluate the sentiment of the debate.\n",
    "\n",
    "### Steps to Follow:\n",
    "1. **Filter the Text**:  \n",
    "   - Include only **candidate responses** by removing any **moderator text** from the analysis.\n",
    "\n",
    "2. **Analyze Sentiment**:  \n",
    "   - Use VADER to calculate the **sentiment score** for each text blurp belonging to the candidates.\n",
    "\n",
    "3. **Create a New Column**:  \n",
    "   - Add a new column in the **full transcript dataframe** to store the **sentiment score** for each blurp.\n",
    "\n",
    "4. **Save the New Dataframe**:  \n",
    "   - Store this modified dataframe as a **separate data structure**.\n",
    "\n",
    "5. **Visualize Sentiment**:  \n",
    "   - Create a **visualization** showing the **sentiment score** of each blurp for each candidate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Q9.** Embeddings\n",
    "Use the **embedding model** provided through `Langchain`'s `HuggingFaceEmbeddings()` to encode the **untokenized text** for each candidate from the data frames.\n",
    "\n",
    "### Steps:\n",
    "1. **Embed Each Candidateâ€™s Speech**:\n",
    "   - For every row in the candidate data frames, embed the untokenized text using the embedding model.\n",
    "   - Return the embeddings in a **list** format.\n",
    "\n",
    "2. **Create a Query**:\n",
    "   - Use the text:  \n",
    "     > *\"I am the Best president in the history of the United States.\"*\n",
    "\n",
    "3. **Embed the Query**:\n",
    "   - Embed the query text using the same model.\n",
    "\n",
    "4. **Find the Most Relevant Document**:\n",
    "   - Use **cosine similarity** to compare the **query embedding** with each candidateâ€™s **speech embeddings**.\n",
    "   - Determine which speech is **most similar** to the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸŸ¢ IMPORTANT: Final Checks\n",
    "\n",
    "Before you submit your homework, please ensure that:\n",
    "- Your complete homework **runs without errors** from top to bottom.  \n",
    "  ðŸ’¡ **Tip**: Use the **Run All** feature to quickly check this.\n",
    "  \n",
    "---\n",
    "\n",
    "## ðŸ“¤ Submission Instructions\n",
    "\n",
    "Submit your completed assignment in both of the following formats:\n",
    "1. **.IPYNB** (Jupyter Notebook format)\n",
    "2. **.HTML** (Webpage format)\n",
    "\n",
    "Upload your files to **Canvas** before the deadline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
